{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most significant changes in the past few years is the shift **FROM** pre-training word embeddings, whether standard ([Mikolov et al., 2013](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf);-- [Pennington et al., 2014](https://www.aclweb.org/anthology/D14-1162.pdf)) or contextualized ([McCann et al., 2017](http://papers.nips.cc/paper/7209-learned-in-translation-contextualized-word-vectors); --  [Peters et al., 2018](https://arxiv.org/abs/1802.05365)) -> **TO** -> to full-network pre-training followed by task-specific fine-tuning ([Dai & Le, 2015](http://papers.nips.cc/paper/5949-semi-supervised-sequence) --   [Devlin et al.; Radford et al.,2018, 2019](https://arxiv.org/abs/1810.04805))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALBERT\n",
    "A Lite BERT (ALBERT) architecture that has significantly fewer parameters than a traditional BERT architecture Thus it addresses memory Limitation Problem. ALBERT incorporates two parameter reduction techniques. \n",
    "* The first one is a factorized embedding parameterization. By decomposing the large vocabulary embedding matrix into two small matrices, we separate the size of the hidden layers from the size of vocabulary embedding. This separation makes it easier to grow the hidden size without significantly increasing the parameter size of the vocabulary embeddings. \n",
    "* The second technique is cross-layer parameter sharing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further improve the performance of ALBERT a **self-supervised loss** for **sentence-order prediction (SOP)** is introduced. SOP primary focuses on inter-sentence coherence and is designed to address the ineffectiveness ([Yang et al., 2019](http://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding); -- [Liu et al., 2019](https://arxiv.org/abs/1907.11692)) of the next sentence prediction (NSP) loss proposed in the original BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
