{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [First Order Motion Model for Image Animation](https://papers.nips.cc/paper/8935-first-order-motion-model-for-image-animation)\n",
    "Image animation refers to the task of automatically synthesizing videos by combining the appearance extracted from\n",
    "a source image with motion patterns derived from a driving video.\n",
    "\n",
    "Image animation consists of generating a video sequence so that an object in a source image is animated according to the motion of a driving video.\n",
    "\n",
    "Once trained on a set of videos depicting objects of the same category (e.g. faces, human bodies), our method can be applied\n",
    "to any object of this class. To achieve this, we decouple appearance and motion information using a self-supervised formulation.\n",
    "\n",
    "To support complex motions, we use a representation consisting of a set of learned keypoints along with their\n",
    "local [affine transformations](https://www.youtube.com/watch?v=il6Z5LCykZk). A generator network models occlusions arising during target motions and combines the appearance extracted from the source image and the motion derived from the driving video.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently, deep generative models have emerged as effective techniques for image animation and\n",
    "video retargeting [2, 41, 3, 42, 27, 28, 37, 40, 31, 21]. In particular, Generative Adversarial Networks\n",
    "(GANs) [14] and Variational Auto-Encoders (VAEs) [20] have been used to transfer facial expressions [37] or motion patterns [3] between human subjects in videos.\n",
    "\n",
    "Nevertheless, these approaches usually rely on pre-trained models in order to extract object-specific representations such as keypoint locations. Unfortunately, these pre-trained models are built using costly ground-truth data annotations [2, 27, 31] and are not available in general for an arbitrary object category. To address this issues, recently Siarohin et al. [28] introduced Monkey-Net, the first object-agnostic deep model for image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Monkey-Net** encodes motion information via keypoints learned in a self-supervised fashion. At test time, the source image is animated according to the corresponding keypoint trajectories estimated in the driving video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../01_03.tvd\", \"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xcc\\xe0\\x01 \\x02\"\\x02 \\x06\"\\x03 \\x0c\"\\x08 \\x17\"\\xe0\"\\t \\x00\"?\\xe1\\x92\\xe1\\xa2\\xe1\\xb2\\xe1\\xc2\\xe1\\xd2\\xe1\\xe2\\xe1?\\xe1\\x92\\xe1\\xa2\\xe1\\xb2\\xe1\\xc2\\xe1\\xd2\\xe1\\xe2\\xe1.`\\ta\\x00b\\xffc\\xffc\\xffc\\xffc\\xffc\\xffc\\xffc\\x99\\x14\\x9a4\\x00\\x02\\x98\\xd4\\x9at\\x02\\x02\\x98\\x94\\x9at\\x04\\x02\\x98\\xd4\\x9at\\x06\\x02\\x99\\x14\\x9a4\\x08\\x02\\x93\\x14\\x93\\xf4\\n'\n"
     ]
    }
   ],
   "source": [
    "for line in f:\n",
    "    print(line)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
