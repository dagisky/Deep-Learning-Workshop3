{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [A simple neural network module for relational reasoning (RN)](https://papers.nips.cc/paper/7082-a-simple-neural-network-module-for-relational-reasoning.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relation Networks (RNs) are a simple plug-and-play module that help to solve problems that fundamentally hinge on relational reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The ability to reason about the relations between entities and their properties is central to generally\n",
    "intelligent behavior [Justin.J, Le Fe](https://arxiv.org/abs/1612.06890),-- [Charles.K, Joshua.B](https://www.pnas.org/content/105/31/10687). Symbolic approaches to artificial intelligence are inherently relational [A. Newell](https://www.sciencedirect.com/science/article/abs/pii/S0364021380800152), [S Harnad](https://www.sciencedirect.com/science/article/abs/pii/0167278990900876). Practitioners define\n",
    "the relations between symbols using the language of logic and mathematics, and then reason about\n",
    "these relations using a multitude of powerful methods, including deduction, arithmetic, and algebra.\n",
    "\n",
    "Deep learning, often struggle in data-poor problems where the underlying structure is characterized by sparse but complex relations [M Garnelo, K Arulkumaran, M Shanahan](https://arxiv.org/abs/1609.05518) -- [BM Lake, TD Ullman, JB Tenenbaum](https://arxiv.org/abs/1604.00289)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relational Reasioning (RN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNs are architectures whose computations focus explicitly on relational reasoning [18]. Although several other models supporting relation-centric computation have been proposed, such as [Graph Neural Neworks](https://ieeexplore.ieee.org/abstract/document/4700287/), [Gated Graph Sequence Neural Netoworks](https://arxiv.org/abs/1511.05493), and [Interaction Networks](http://papers.nips.cc/paper/6417-interaction-networks-for-learning-about-objects-relations-and-physics), RNs are simpler, more exclusively focused on general relation reasoning, and easier to integrate within broader architectures.\n",
    "\n",
    "The design philosophy behind RNs is to constrain the functional form of a neural network so that it captures the\n",
    "core common properties of relational reasoning. In other words, the capacity to compute relations is baked into the RN architecture without needing to be learned, just as the capacity to reason about spatial, translation invariant properties is built-in to CNNs, and the capacity to reason about sequential dependencies is built into recurrent neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$RN(O) = f_\\Phi(\\sum_{i,j} g_\\theta(o_i, o_j))$ \n",
    "\n",
    "Where the input is a set of “objects” $O = \\{o_1, o_2, ..., o_n\\}$, $o_i ∈ R^m$ is the $i^{th}$ object, and $f_φ$ and $g_θ$ are functions with parameters $φ$ and $θ$, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationalLayerBase(nn.Module):\n",
    "    \"\"\"\n",
    "    The relational Base Layer \n",
    "    An RN is a neural network module with a structure primed for relational reasoning. The design\n",
    "    philosophy behind RNs is to constrain the functional form of a neural network so that it captures the\n",
    "    core common properties of relational reasoning.\n",
    "    \n",
    "    Child of nn.Module Class\n",
    "    \"\"\"\n",
    "    def __init__(self, in_size, out_size, device, hyp):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Relational Base Layer Initializer\n",
    "        Args:\n",
    "            in_size (Integer): network input size\n",
    "            out_size (Integer): network output size\n",
    "            device (torch.device): Pytorch Device Object\n",
    "            hyp (Dictionary): Hyperparmeters {g_layers:<Int>, f_fc1:<Int>, f_fc2:<Int>, dropout:<float>}\n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.f_fc1 = nn.Linear(hyp[\"g_layers\"][-1], hyp[\"f_fc1\"]).to(device)\n",
    "        self.f_fc2 = nn.Linear(hyp[\"f_fc1\"], hyp[\"f_fc2\"]).to(device)\n",
    "        self.f_fc3 = nn.Linear(hyp[\"f_fc2\"], out_size).to(device)\n",
    "    \n",
    "        self.dropout = nn.Dropout(p=hyp[\"dropout\"])\n",
    "        \n",
    "        self.on_gpu = True\n",
    "        self.hyp = hyp\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "\n",
    "    def cuda(self):\n",
    "        \"\"\"Load Model on CUDA\"\"\"\n",
    "        self.on_gpu = True\n",
    "        super().cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[[1,2,3][3,4,5]]] # [B x 2 x 3] here B=1, sequence = 2 and feature_size = 3\n",
    "# Now Let us create x_i\n",
    "x_i = [[ [[1,2,3],[4,5,6]] ]] # [B x 1 x 2 x 3] \n",
    "x_i = [[ [[1,2,3],[4,5,6]],[[1,2,3],[4,5,6]] ]] # [B x 2 x 2 x 3] as you can see the whole sequence is repeated\n",
    "\n",
    "x_j = [[ [[1,2,3]],[[4,5,6]] ]] # [B x 2 x 1 x 3] \n",
    "x_j = [[ [[1,2,3],[1,2,3]],[[4,5,6],[4,5,6]] ]] # [B x 2 x 2 x 3] In this case the features are repeated\n",
    "\n",
    "# The Last Step is to Concatinate x_i and x_j on the last dimention and pass them through Linear unit with RELU\n",
    "cat = [[ [[1,2,3,1,2,3],[4,5,6,1,2,3]],[[1,2,3,4,5,6],[4,5,6,4,5,6]] ]] # [B x 1 x 2 x 6] \n",
    "# NOTE: When we are concatinating x_i and x_j we are mapping each sequence of X with each feature within it.\n",
    "# In other words each features of x_j are mapped with all sequences of x_i \n",
    "# The first sequence of x_j [1,2,3] is mapped with all the sequences of X, that is [1,2,3] and [3,4,5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationalLayer(RelationalLayerBase):\n",
    "    \"\"\"\n",
    "    Child Class of RelationalLayerBase\n",
    "    Creates the g_layers for the given a set of “objects” O = {o1, o2, ..., on}\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        g_layers : list[nn.Linear]\n",
    "            List of nn.Linear modules \n",
    "        edge_feature: nn.Linear Module\n",
    "            Transform the output feature\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, in_size, out_size, device, hyp):\n",
    "        super().__init__(in_size, out_size, device, hyp)\n",
    "        \"\"\"\n",
    "        Relational Layer Initializer\n",
    "        Args:\n",
    "            in_size (Integer): network input size\n",
    "            out_size (Integer): network output size\n",
    "            device (torch.device): Pytorch Device Object\n",
    "            hyp (Dictionary): Hyperparmeters {g_layers:<Int>, f_fc1:<Int>, f_fc2:<Int>, dropout:<float>}\n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        self.in_size = in_size\n",
    "        self.edge_feature = nn.Linear((in_size//2)*4, in_size).to(device)\n",
    "\n",
    "        #create all g layers\n",
    "        self.g_layers = []\n",
    "        self.g_layers_size = hyp[\"g_layers\"]\n",
    "\n",
    "        for idx, g_layer_size in enumerate(hyp[\"g_layers\"]):\n",
    "            in_s = in_size if idx==0 else hyp[\"g_layers\"][idx-1]\n",
    "            out_s = g_layer_size\n",
    "            l = nn.Linear(in_s, out_s).to(self.device)\n",
    "            self.g_layers.append(l) \n",
    "        self.g_layers = nn.ModuleList(self.g_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Implements the forward method of nn.Module Class\n",
    "        Args:\n",
    "            x(Tensor): batch_size x seqence_size x feature_size\n",
    "        Returns:\n",
    "            Tensor\n",
    "        \"\"\"\n",
    "        b, d, k = x.size()  # here b=Batch_size, d=sequence_size, k=feature_size \n",
    "\n",
    "        # cast all pairs against each other\n",
    "        x_i = torch.unsqueeze(x, 1)                  # (B x 1 x 64 x 26)\n",
    "        x_i = x_i.repeat(1, d, 1, 1)                 # (B x 64 x 64 x 26)\n",
    "        x_j = torch.unsqueeze(x, 2)                  # (B x 64 x 1 x 26)\n",
    "        x_j = x_j.repeat(1, 1, d, 1)                 # (B x 64 x 64 x 26)\n",
    "        \"\"\"Note: as shown above x_i and x_j repeat the sequence and the feature 64 times respectively.\n",
    "        This means \"\"\"\n",
    "        \n",
    "        # concatenate all together\n",
    "        x_full = torch.cat([x_i, x_j], 3)        # (B x 64 x 64 x 2*26)\n",
    "        x_full = self.edge_feature(x_full)       # (B x 64 x 64 x 2*26)\n",
    "        \n",
    "        # reshape for passing through network\n",
    "        x_ = x_full.view(b * d**2, self.in_size)\n",
    "\n",
    "        for idx, (g_layer, g_layer_size) in enumerate(zip(self.g_layers, self.g_layers_size)):          \n",
    "            x_ = g_layer(x_)\n",
    "            x_ = F.relu(x_)\n",
    "\n",
    "        # reshape again and sum\n",
    "        x_g = x_.view(b, d**2, self.g_layers_size[-1])\n",
    "        x_g = x_g.sum(1).squeeze(1)\n",
    "        \n",
    "        \"\"\"f\"\"\"\n",
    "        x_f = self.f_fc1(x_g)\n",
    "        x_f = F.relu(x_f)\n",
    "        x_f = self.f_fc2(x_f)\n",
    "        x_f = self.dropout(x_f)\n",
    "        x_f = F.relu(x_f)\n",
    "        x_f = self.f_fc3(x_f)\n",
    "\n",
    "        return F.log_softmax(x_f, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "#### bAbI\n",
    "RN **succeeded on 18/20** tasks. Notably, it succeeded on the basic induction task (2.1% total error).\n",
    "Also, RN did not catastrophically fail in any of the tasks: for the 2 tasks that it failed (the **“two supporting facts”**, and **“three supporting facts”** tasks), it **missed the 95% threshold** by **3.1% and 11.5%**, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
