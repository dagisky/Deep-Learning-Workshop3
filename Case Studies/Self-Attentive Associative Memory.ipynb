{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "###  Outer product attention (OPA)\n",
    "Outer product attention (OPA) is a natural extension of the query-key-value dot product attention (Vaswani et al. 2017). Dot product attention (DPA) for single query $q$ and $n_{kv}$ pairs of key-value can be formulated as follows   \n",
    "<font size=\"4\">$A^o(q, K, V) = \\sum_{i=1}^{n_{kv}}S(q.k_i)v_i$</font>  \n",
    "Where $A^o ∈ R^{d_v}, v_i ∈ R^{d_v}$ and  $q,k_i ∈ R^{qk}$\n",
    "\n",
    "**Here new outer product attention is proposed.**   \n",
    "<font size=\"4\">$A^⊗(q, K, V) = \\sum_{i=1}^{n_{kv}}F(q⊙k_i)⊗v_i$</font>  \n",
    "Where $A^⊗ ∈ R^{d_{qk} x d_v}, v_i ∈ R^{d_v}$ and  $q,k_i ∈ R^{qk}$  \n",
    "And ⊙ is element-wise multiplication, ⊗ is outer product and $F$ is chosen as element-wise $tanh$ function.  \n",
    "A crucial difference between DPA (Dot Product Attention) and OPA is that while the former retrieves an attended item $A^o$ , the latter forms a relational representation $A^⊗$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def op_att(q, k, v):\n",
    "    print(q.size())\n",
    "    print(k.size()) # [b x s x  k]\n",
    "    print(v.size()) # [b x s x v]\n",
    "    qq = q.unsqueeze(2).repeat(1, 1, k.shape[1], 1) # [b x s x s x k]\n",
    "    kk = k.unsqueeze(1).repeat(1, q.shape[1], 1, 1) # [b x s x s x v]\n",
    "    output = torch.matmul(F.tanh(qq*kk).unsqueeze(4), v.unsqueeze(1).repeat(1, q.shape[1], 1, 1).unsqueeze(3))  # BxNXNxd_kq BxNxNxd_v --> BxNXNxd_kqxd_v\n",
    "    # print(output.shape)\n",
    "    output = torch.sum(output, dim=2)  # BxNxd_kqxd_v\n",
    "    # print(output.shape)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10, 10, 7]) torch.Size([5, 10, 10, 7])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(5, 10, 7)\n",
    "b = torch.rand(5,  10, 7)\n",
    "v = torch.rand(5, 10, 7)\n",
    "aa = a.unsqueeze(2).repeat(1, 1, a.shape[1], 1) # [b x s x s x k]\n",
    "bb = b.unsqueeze(1).repeat(1, b.shape[1], 1, 1) # [b x s x s x v]\n",
    "print(aa.size(), bb.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 10, 7, 7])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = torch.matmul(F.tanh(aa*bb).unsqueeze(4), v.unsqueeze(1).repeat(1, a.shape[1], 1, 1).unsqueeze(3)) \n",
    "output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SAM](Images/SAM.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$M^r$ -Read As relationships stored in $M^r$ are represented as associative memories, the relational memory can be read to reconstruct previously seen item.\n",
    "<font size=\"4\">$v^r_t = softmax(f_3(x_t)^T)M^r_{t-1}f_2(x_t)$</font>  \n",
    "Where $f_3$ is feed-forward neural networks that outputs a $n_q$-dimentional vector. The read value provides an additional input coming from the previous state of $M^r$ to relational construction process as follows.  \n",
    "<font size=\"4\">$M^r_t = M^r_{t-1} + \\alpha_1SAM_θ(M^i_t + \\alpha_2v^r_t⊗f_2(x_2))$</font>  \n",
    "Where $\\alpha_1$ and $\\alpha_2$ are blending hyper-parameters.  \n",
    "$M^i$-Read, $M^r$-Write: SAM is used to read from $M^i$ and construct a condidate relational memory, which is simply added to the previous relational memory to perform the relational update.  \n",
    "The input for SAM is the combination of the current item memory $M^i_t$ and the association between the extracted item from the previous relational memory $v^r_t$ and the current input data $x_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim=28*28,  out_dim=10, hid_dim=-1, layers=1):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = layers\n",
    "        if hid_dim<=0:\n",
    "            self.layers=-1\n",
    "        if self.layers<0:\n",
    "            hid_dim=out_dim\n",
    "        self.fc1 = nn.Linear(in_dim, hid_dim)\n",
    "        # linear layer (n_hidden -> hidden_2)\n",
    "        if self.layers>0:\n",
    "            self.fc2h = nn.ModuleList([nn.Linear(hid_dim, hid_dim)]*self.layers)\n",
    "        # linear layer (n_hidden -> 10)\n",
    "        if self.layers>=0:\n",
    "            self.fc3 = nn.Linear(hid_dim, out_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        o = self.fc1(x)\n",
    "        if self.layers>0:\n",
    "            for l in range(self.layers):\n",
    "                o = self.fc2h[l](o)\n",
    "        if self.layers >= 0:\n",
    "            o = self.fc3(o)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In STM, at every timestep, the item memory $M^i_t$ is updated with new input $x_t$ using gating mechanisms as follows.  \n",
    "<font size=\"5\"> $M^i_t = F_t(M^i_{t-1}, x_t)⊙M^i_{t-1}+I_t(M^i_{t-1},x_t)⊙X_t$  </font>  \n",
    "Where $F_t$ and $I_t$ are the input and forget gates, respectively  \n",
    "<font size=\"3\">$F_t(M^i_{t-1})=W_Fx_t + U_Ftanh(M^i_{t-1}) + b_F$</font>  \n",
    "<font size=\"3\">$I_t(M^i_{t-1}, x_t) = W_Ix_t + U_Itanh(M^i_{t-1}) + b_I$</font>  \n",
    "Here, $W_F, U_F, W_I ∈ R^{dxd}$ are parametric weights and $b_F$, $b_I$ ∈ R are the biases and + is broadcasted if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$M^r$-Read** As relationships stored in $M^r$ are represented as associative memories, the relational memory can be read to reconstruct previously seen items.  \n",
    "<font size=\"3\">$v^r_t = softmax(f_3(x_t)^T)M^r_{t-1}f_2$</font>  \n",
    "Where $f_3$ is feed-forward neural network that outputs $n_q$-dimentional vector. The read value provides an additional input comming from the previous state of $M^r$ to relational construction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STM(nn.Module):\n",
    "    def __init__(self, input_size, output_size, step = 1, num_slot=8,\n",
    "                 mlp_size = 128, slot_size = 96, rel_size = 96,\n",
    "                 out_att_size=64, rd=True,\n",
    "                 init_alphas=[None,None,None],\n",
    "                 learn_init_mem=True, mlp_hid=-1):\n",
    "        super(STM, self).__init__()\n",
    "        self.mlp_size = mlp_size # 128\n",
    "        self.slot_size = slot_size # 96\n",
    "        self.rel_size = rel_size # 96\n",
    "        self.rnn_hid = slot_size # 96\n",
    "        self.num_slot = num_slot # 8\n",
    "        self.step = step # 1\n",
    "        self.rd = rd # True\n",
    "        self.learn_init_mem = learn_init_mem # True\n",
    "\n",
    "        self.out_att_size = out_att_size # 64\n",
    "        \n",
    "        #==================== create qkv attention projectors for each step ==============\n",
    "\n",
    "        self.qkv_projector = nn.ModuleList([nn.Linear(slot_size, num_slot*3)]*step) # [96 -> 24]\n",
    "        self.qkv_layernorm = nn.ModuleList([nn.LayerNorm([slot_size, num_slot*3])]*step)\n",
    "    \n",
    "        #=================== create alpha values =========================\n",
    "        if init_alphas[0] is None:\n",
    "            self.alpha1 = [nn.Parameter(torch.zeros(1))] * step\n",
    "            for ia, a in enumerate(self.alpha1):\n",
    "                setattr(self, 'alpha1' + str(ia), self.alpha1[ia])\n",
    "        else:\n",
    "            self.alpha1 = [init_alphas[0]]* step\n",
    "\n",
    "        if init_alphas[1] is None:\n",
    "            self.alpha2 = [nn.Parameter(torch.zeros(1))] * step\n",
    "            for ia, a in enumerate(self.alpha2):\n",
    "                setattr(self, 'alpha2' + str(ia), self.alpha2[ia])\n",
    "        else:\n",
    "            self.alpha2 = [init_alphas[1]] * step\n",
    "\n",
    "        if init_alphas[2] is None:\n",
    "            self.alpha3 = [nn.Parameter(torch.zeros(1))] * step\n",
    "            for ia, a in enumerate(self.alpha3):\n",
    "                setattr(self, 'alpha3' + str(ia), self.alpha3[ia])\n",
    "        else:\n",
    "            self.alpha3 = [init_alphas[2]] * step\n",
    "\n",
    "\n",
    "        self.input_projector = MLP(input_size, slot_size, hid_dim=mlp_hid)  # [feat_size x 96, -1]\n",
    "        self.input_projector2 = MLP(input_size, slot_size, hid_dim=mlp_hid)  # [feat_size x 96, -1]\n",
    "        self.input_projector3 = MLP(input_size, num_slot, hid_dim=mlp_hid)  # [feat_size x 8, -1]\n",
    "\n",
    "\n",
    "        self.input_gate_projector = nn.Linear(self.slot_size, self.slot_size*2) # [96  x 184]\n",
    "        self.memory_gate_projector = nn.Linear(self.slot_size, self.slot_size*2) # [96  x 184]\n",
    "        \n",
    "        # trainable scalar gate bias tensors\n",
    "        self.forget_bias = nn.Parameter(torch.tensor(1., dtype=torch.float32))\n",
    "        self.input_bias = nn.Parameter(torch.tensor(0., dtype=torch.float32))\n",
    "\n",
    "        self.rel_projector = nn.Linear(slot_size * slot_size, rel_size) # [96 x 96, 96]\n",
    "        self.rel_projector2 = nn.Linear(num_slot * slot_size, slot_size) # [8 x 96, 96]\n",
    "        self.rel_projector3 = nn.Linear(num_slot * rel_size, out_att_size) # [8 x 96, 64]\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(out_att_size, self.mlp_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.mlp_size, self.mlp_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(self.mlp_size, output_size)\n",
    "        \n",
    "        # Initialize memory units \n",
    "        # item_memory_state_bias=[96x96]\n",
    "        # rel_memory_state_bias=[8x96x96]\n",
    "        if self.learn_init_mem:\n",
    "            if not torch.cuda.is_available():\n",
    "                self.register_parameter('item_memory_state_bias',\n",
    "                                        torch.nn.Parameter(torch.Tensor(self.slot_size, self.slot_size).cuda()))\n",
    "                self.register_parameter('rel_memory_state_bias', torch.nn.Parameter(\n",
    "                    torch.Tensor(self.num_slot, self.slot_size, self.slot_size).cuda()))\n",
    "\n",
    "            else:\n",
    "                self.register_parameter('item_memory_state_bias',\n",
    "                                        torch.nn.Parameter(torch.Tensor(self.slot_size, self.slot_size)))\n",
    "                self.register_parameter('rel_memory_state_bias',\n",
    "                                        torch.nn.Parameter(torch.Tensor(self.num_slot, self.slot_size, self.slot_size)))\n",
    "\n",
    "            stdev = 1 / (np.sqrt(self.slot_size + self.slot_size))\n",
    "            nn.init.uniform_(self.item_memory_state_bias, -stdev, stdev)\n",
    "            stdev = 1 / (np.sqrt(self.slot_size + self.slot_size + self.num_slot))\n",
    "            nn.init.uniform_(self.rel_memory_state_bias, -stdev, stdev)\n",
    "\n",
    "\n",
    "    def create_new_state(self, batch_size):\n",
    "        \"\"\"Create new State with batch_size of b\"\"\"\n",
    "        if self.learn_init_mem: # True\n",
    "            read_heads = torch.zeros(batch_size, self.out_att_size) # Read Heads per batch_size, out_att_size=64 [bs x 64]\n",
    "            item_memory_state = self.item_memory_state_bias.clone().repeat(batch_size, 1, 1) # [bs x 96 x 96]\n",
    "            rel_memory_state = self.rel_memory_state_bias.clone().repeat(batch_size, 1, 1, 1) # [bs x 8 x 96 x 96]\n",
    "            if not torch.cuda.is_available():\n",
    "                read_heads = read_heads.cuda()\n",
    "        else:\n",
    "            item_memory_state =  torch.stack([torch.zeros(self.slot_size, self.slot_size) for _ in range(batch_size)])\n",
    "            read_heads =  torch.zeros(batch_size, self.out_att_size)\n",
    "            rel_memory_state =  torch.stack([torch.zeros(self.num_slot, self.slot_size, self.slot_size) for _ in range(batch_size)])\n",
    "            if not torch.cuda.is_available():\n",
    "                item_memory_state = item_memory_state.cuda()\n",
    "                read_heads = read_heads.cuda()\n",
    "                rel_memory_state = rel_memory_state.cuda()\n",
    "\n",
    "        return read_heads, item_memory_state, rel_memory_state # [bs x 64], [bs x 96x96],  [bs x 8x96x96]\n",
    "\n",
    "\n",
    "\n",
    "    def compute_gates(self, inputs, memory):\n",
    "        # inputs = [feat_size x 1  x 96], memory = [bs x 96 x 96]\n",
    "        memory = torch.tanh(memory)\n",
    "        if len(inputs.shape) == 3:\n",
    "            if inputs.shape[1] > 1:\n",
    "                raise ValueError(\n",
    "                    \"input seq length is larger than 1. create_gate function is meant to be called for each step, with input seq length of 1\")\n",
    "            inputs = inputs.view(inputs.shape[0], -1) # [feat_size  x 96]\n",
    "\n",
    "            gate_inputs = self.input_gate_projector(inputs) # [feat_size  x 184]\n",
    "            gate_inputs = gate_inputs.unsqueeze(dim=1) # [feat_size  x 1 x 184]\n",
    "            gate_memory = self.memory_gate_projector(memory) # [bs x 96  x 184]\n",
    "        else:\n",
    "            raise ValueError(\"input shape of create_gate function is 2, expects 3\")\n",
    "\n",
    "        gates = gate_memory + gate_inputs\n",
    "        gates = torch.split(gates, split_size_or_sections=int(gates.shape[2] / 2), dim=2)\n",
    "        input_gate, forget_gate = gates\n",
    "        assert input_gate.shape[2] == forget_gate.shape[2]\n",
    "\n",
    "        input_gate = torch.sigmoid(input_gate + self.input_bias)\n",
    "        forget_gate = torch.sigmoid(forget_gate + self.forget_bias)\n",
    "\n",
    "        return input_gate, forget_gate\n",
    "\n",
    "    def compute(self, input_step, prev_state):\n",
    "        # [bs x feat_size], [[bs x 64], [bs x 96x96],  [bs x 8x96x96]]\n",
    "        hid = prev_state[0] # read_heads [bs x 64]\n",
    "        item_memory_state = prev_state[1] # [bs x 96x96]\n",
    "        rel_memory_state = prev_state[2] #  [bs x 8x96x96]\n",
    "\n",
    "        #transform input\n",
    "        controller_outp = self.input_projector(input_step) # [feat_size x 96]\n",
    "        controller_outp2 = self.input_projector2(input_step) # [feat_size x 96]\n",
    "        controller_outp3 = self.input_projector3(input_step)  # [feat_size x 8]\n",
    "\n",
    "\n",
    "        #Mi write\n",
    "        X = torch.matmul(controller_outp.unsqueeze(2), controller_outp.unsqueeze(1))  #[10, 96, 96] Bxdxd [feat_size x 96, 1]x [feat_size x 1  x 96]\n",
    "        input_gate, forget_gate = self.compute_gates(controller_outp.unsqueeze(1), item_memory_state) # [feat_size x 1  x 96] [bs x 96x96]\n",
    "\n",
    "\n",
    "        #Mr read\n",
    "        controller_outp3 = F.softmax(controller_outp3, dim=-1)\n",
    "        controller_outp4 = torch.einsum('bn,bd,bndf->bf', controller_outp3, controller_outp2, rel_memory_state)\n",
    "        X2 = torch.einsum('bd,bf->bdf', controller_outp4, controller_outp2)\n",
    "\n",
    "        if self.rd:\n",
    "            # Mi write gating\n",
    "            R = input_gate * F.tanh(X)\n",
    "            R += forget_gate * item_memory_state\n",
    "        else:\n",
    "            #Mi write\n",
    "            R =  item_memory_state + torch.matmul(controller_outp.unsqueeze(2), controller_outp.unsqueeze(1))#Bxdxd\n",
    "\n",
    "        for i in range(self.step):\n",
    "            #SAM\n",
    "            qkv = self.qkv_projector[i](R+self.alpha2[i]*X2)\n",
    "            qkv = self.qkv_layernorm[i](qkv)\n",
    "            qkv = qkv.permute(0,2,1) #Bx3Nxd\n",
    "\n",
    "            q,k,v = torch.split(qkv, [self.num_slot]*3, 1)#BxNxd\n",
    "\n",
    "\n",
    "            R0 = op_att(q, k, v) #BxNxdxd\n",
    "\n",
    "            #Mr transfer to Mi\n",
    "            R2= self.rel_projector2(R0.view(R0.shape[0], -1, R0.shape[3]).permute(0, 2, 1))\n",
    "            R =  R + self.alpha3[i] * R2\n",
    "\n",
    "            #Mr write\n",
    "            rel_memory_state = self.alpha1[i]*rel_memory_state + R0\n",
    "\n",
    "        #Mr transfer to output\n",
    "        r_vec = self.rel_projector(rel_memory_state.view(rel_memory_state.shape[0],\n",
    "                                                         rel_memory_state.shape[1],\n",
    "                                                         -1)).view(input_step.shape[0],-1)\n",
    "        out = self.rel_projector3(r_vec)\n",
    "\n",
    "        # if self.gating_after:\n",
    "        #     #Mi write gating\n",
    "        #     input_gate, forget_gate = self.compute_gates(controller_outp.unsqueeze(1), R)\n",
    "        #     if self.rd:\n",
    "        #         R = input_gate * torch.tanh(R)\n",
    "        #         R += forget_gate * item_memory_state\n",
    "\n",
    "        return out, (out, R, rel_memory_state)\n",
    "\n",
    "    def forward(self, input_step, hidden=None): # input_step=> [seq_size, batch_size, feature_size]\n",
    "\n",
    "        if len(input_step.shape)==3:\n",
    "            self.init_sequence(input_step.shape[1])#   self.previous_state = > [bs x 64], [bs x 96x96],  [bs x 8x96x96]\n",
    "            for i in range(input_step.shape[0]): # for every sequence [word]\n",
    "                logit, self.previous_state = self.compute(input_step[i], self.previous_state) # [bs x feat_size], [[bs x 64], [bs x 96x96],  [bs x 8x96x96]]\n",
    "\n",
    "        else:\n",
    "            if hidden is not None:\n",
    "                logit, hidden = self.compute(input_step, hidden)\n",
    "            else:\n",
    "                logit, self.previous_state = self.compute(input_step,  self.previous_state)\n",
    "        mlp = self.mlp(logit)\n",
    "        out = self.out(mlp)\n",
    "        return out, self.previous_state\n",
    "\n",
    "    def init_sequence(self, batch_size):\n",
    "        \"\"\"Initializing the state.\"\"\"\n",
    "        self.previous_state = self.create_new_state(batch_size) # create new state [bs x 64], [bs x 96x96],  [bs x 8x96x96]\n",
    "\n",
    "    def calculate_num_params(self):\n",
    "        \"\"\"Returns the total number of parameters.\"\"\"\n",
    "        num_params = 0\n",
    "        for p in self.parameters():\n",
    "            num_params += p.data.view(-1).size(0)\n",
    "        return num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "stm = STM(10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(10,96) # [feat_size x 96, 1]x [feat_size x 1  x 96]\n",
    "b = torch.matmul(a.unsqueeze(2), a.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 96, 96])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.tanh(b)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 8, 96])\n",
      "torch.Size([7, 8, 96])\n",
      "torch.Size([7, 8, 96])\n",
      "torch.Size([7, 8, 96])\n",
      "torch.Size([7, 8, 96])\n",
      "torch.Size([7, 8, 96])\n",
      "torch.Size([7, 8, 96])\n",
      "torch.Size([7, 8, 96])\n",
      "torch.Size([7, 8, 96])\n",
      "torch.Size([7, 8, 96])\n",
      "torch.Size([7, 8, 96])\n",
      "torch.Size([7, 8, 96])\n",
      "torch.Size([7, 8, 96])\n",
      "torch.Size([7, 8, 96])\n",
      "torch.Size([7, 8, 96])\n"
     ]
    }
   ],
   "source": [
    " b = stm(a, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha1 = nn.Parameter(torch.tensor(1., dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem1 = torch.nn.Parameter(torch.Tensor(96, 96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([96, 96])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem1.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention Transformer Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [International Conference on NLP Techniques and Applications- Sep 5](http://www.wikicfp.com/cfp/servlet/event.showcfp?eventid=106242&copyownerid=33993)\n",
    "### [13th International Conference on Agents and Artificial Intelligence - Sep 14](http://www.wikicfp.com/cfp/servlet/event.showcfp?eventid=105618&copyownerid=45217)\n",
    "Heretofore, neural networks with external memory are restricted to single memory with lossy representations of memory interactions. A rich representation of relationships between memory pieces urges a high-order and segregated relational memory.\n",
    "\n",
    "A rich representation of relationships between memory pieces urges a high-order and segregated relational memory. In this paper, we propose to separate the storage of individual experiences (item memory) and their occurring relationships (relational memory). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model proposed here creates item memory based on attention. The attention model is used to read and write over the memory module. The module uses gates to read and write over attended sections.  \n",
    "The input is used to create relational reasoning (outer product to each memory section with given attention). The attention should we weighted to understand which memory section to select based on (key query value) attention mechanism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Implement the PE function.\"\"\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"\"\"Compute Scaled Dot Product Attention\"\"\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1))/ math.sqrt(d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"\"\"Take in model size and number of heads.\"\"\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"Implements Figure 2\"\"\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Core encoder is a stack of N layers\"\"\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"Pass the input (and mask) through each layer in turn.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"Construct a layernorm module (See citation for details). [Norm]\"\"\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"Encoder is made up of self-attn and feed forward (defined below)\"\"\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "        self.slot_size = size\n",
    "        self.num_slot = 5\n",
    "        \n",
    "        # ============= init memory model ================        \n",
    "        if self.learn_init_mem:\n",
    "            if not torch.cuda.is_available():\n",
    "                self.register_parameter('item_memory_state_bias',\n",
    "                                        torch.nn.Parameter(torch.Tensor(self.num_slot, self.slot_size).cuda()))         \n",
    "\n",
    "            else:\n",
    "                self.register_parameter('item_memory_state_bias',\n",
    "                                        torch.nn.Parameter(torch.Tensor(self.num_slot, self.slot_size)))\n",
    "                \n",
    "            stdev = 1 / (np.sqrt(self.num_slot + self.slot_size))\n",
    "            nn.init.uniform_(self.item_memory_state_bias, -stdev, stdev)\n",
    "        \n",
    "    def compute_gates(self, inputs, memory): # compute gates as LSTM on attention mechanism\n",
    "        # inputs = [feat_size x 1  x 96], memory = [bs x 96 x 96]\n",
    "        memory = torch.tanh(memory)\n",
    "        if len(inputs.shape) == 3:\n",
    "            if inputs.shape[1] > 1:\n",
    "                raise ValueError(\n",
    "                    \"input seq length is larger than 1. create_gate function is meant to be called for each step, with input seq length of 1\")\n",
    "            inputs = inputs.view(inputs.shape[0], -1) # [feat_size  x 96]\n",
    "\n",
    "            gate_inputs = self.input_gate_projector(inputs) # [feat_size  x 184]\n",
    "            gate_inputs = gate_inputs.unsqueeze(dim=1) # [feat_size  x 1 x 184]\n",
    "            gate_memory = self.memory_gate_projector(memory) # [bs x 96  x 184]\n",
    "        else:\n",
    "            raise ValueError(\"input shape of create_gate function is 2, expects 3\")\n",
    "\n",
    "        gates = gate_memory + gate_inputs\n",
    "        gates = torch.split(gates, split_size_or_sections=int(gates.shape[2] / 2), dim=2)\n",
    "        input_gate, forget_gate = gates\n",
    "        assert input_gate.shape[2] == forget_gate.shape[2]\n",
    "\n",
    "        input_gate = torch.sigmoid(input_gate + self.input_bias)\n",
    "        forget_gate = torch.sigmoid(forget_gate + self.forget_bias)\n",
    "\n",
    "        return input_gate, forget_gate\n",
    "    \n",
    "    def compute(self, input_step, prev_state):\n",
    "        # [bs x feat_size], [[bs x 64], [bs x 96x96],  [bs x 8x96x96]]\n",
    "        hid = prev_state[0] # read_heads [bs x 64]\n",
    "        item_memory_state = prev_state[1] # [bs x 96x96]\n",
    "        rel_memory_state = prev_state[2] #  [bs x 8x96x96]\n",
    "\n",
    "        #transform input\n",
    "        controller_outp = self.input_projector(input_step) # [feat_size x 96]\n",
    "        controller_outp2 = self.input_projector2(input_step) # [feat_size x 96]\n",
    "        controller_outp3 = self.input_projector3(input_step)  # [feat_size x 8]\n",
    "\n",
    "\n",
    "        #Mi write\n",
    "        X = torch.matmul(controller_outp.unsqueeze(2), controller_outp.unsqueeze(1))  #[10, 96, 96] Bxdxd [feat_size x 96, 1]x [feat_size x 1  x 96]\n",
    "        input_gate, forget_gate = self.compute_gates(controller_outp.unsqueeze(1), item_memory_state) # [feat_size x 1  x 96] [bs x 96x96]\n",
    "\n",
    "\n",
    "        #Mr read\n",
    "        controller_outp3 = F.softmax(controller_outp3, dim=-1)\n",
    "        controller_outp4 = torch.einsum('bn,bd,bndf->bf', controller_outp3, controller_outp2, rel_memory_state)\n",
    "        X2 = torch.einsum('bd,bf->bdf', controller_outp4, controller_outp2)\n",
    "\n",
    "        if self.rd:\n",
    "            # Mi write gating\n",
    "            R = input_gate * F.tanh(X)\n",
    "            R += forget_gate * item_memory_state\n",
    "        else:\n",
    "            #Mi write\n",
    "            R =  item_memory_state + torch.matmul(controller_outp.unsqueeze(2), controller_outp.unsqueeze(1))#Bxdxd\n",
    "\n",
    "        for i in range(self.step):\n",
    "            #SAM\n",
    "            qkv = self.qkv_projector[i](R+self.alpha2[i]*X2)\n",
    "            qkv = self.qkv_layernorm[i](qkv)\n",
    "            qkv = qkv.permute(0,2,1) #Bx3Nxd\n",
    "\n",
    "            q,k,v = torch.split(qkv, [self.num_slot]*3, 1)#BxNxd\n",
    "\n",
    "\n",
    "            R0 = op_att(q, k, v) #BxNxdxd\n",
    "\n",
    "            #Mr transfer to Mi\n",
    "            R2= self.rel_projector2(R0.view(R0.shape[0], -1, R0.shape[3]).permute(0, 2, 1))\n",
    "            R =  R + self.alpha3[i] * R2\n",
    "\n",
    "            #Mr write\n",
    "            rel_memory_state = self.alpha1[i]*rel_memory_state + R0\n",
    "\n",
    "        #Mr transfer to output\n",
    "        r_vec = self.rel_projector(rel_memory_state.view(rel_memory_state.shape[0],\n",
    "                                                         rel_memory_state.shape[1],\n",
    "                                                         -1)).view(input_step.shape[0],-1)\n",
    "        out = self.rel_projector3(r_vec)\n",
    "\n",
    "        # if self.gating_after:\n",
    "        #     #Mi write gating\n",
    "        #     input_gate, forget_gate = self.compute_gates(controller_outp.unsqueeze(1), R)\n",
    "        #     if self.rd:\n",
    "        #         R = input_gate * torch.tanh(R)\n",
    "        #         R += forget_gate * item_memory_state\n",
    "\n",
    "        return out, (out, R, rel_memory_state)\n",
    "\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"Follow Figure 1 (left) for connections.\"\"\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_encoder_model(src_vocab, tgt_vocab, N=6, \n",
    "               d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N)\n",
    "\n",
    "    # model = EncoderDecoder(\n",
    "    #     Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "    #     Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
    "    #                          c(ff), dropout), N),\n",
    "    #     nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "    #     nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "    #     Generator(d_model, tgt_vocab))\n",
    "    \n",
    "    # This was important from their code. \n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
